{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning:\n",
    "\n",
    "We faced a wide array of bugs when attempting to utilize the script out of the box. There will be code cells that was used for testing out bugs and isolating them down. Please disregard them. We have attempted to remove as many as possible, but we may have missed a few. The script, however, should work properly now despite our initial troubles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import unicodedata\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import math\n",
    "from config import Config\n",
    "import dateutil.relativedelta\n",
    "import pandas_market_calendars as mcal\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "class Timeout():\n",
    "    class TimeoutException(Exception):\n",
    "        pass\n",
    "\n",
    "    def _timeout(signum, frame):\n",
    "        raise Timeout.TimeoutException()\n",
    "\n",
    "    def __init__(self, timeout=10):\n",
    "        self.timeout = timeout\n",
    "        signal.signal(signal.SIGALRM, Timeout._timeout)\n",
    "\n",
    "    def __enter__(self):\n",
    "        signal.alarm(self.timeout)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        signal.alarm(0)\n",
    "        return exc_type is Timeout.TimeoutException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Define Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import unicodedata\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import re\n",
    "import requests\n",
    "\n",
    "class SEC_Extractor:\n",
    "    def get_doc_links(cik,ticker):\n",
    "        try:\n",
    "            base_url = \"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "            inputted_cik = cik\n",
    "            payload = {\n",
    "                \"action\" : \"getcompany\",\n",
    "                \"CIK\" : inputted_cik,\n",
    "                \"type\" : \"8-K\",\n",
    "                \"output\":\"xml\",\n",
    "                \"dateb\" : \"20180401\",\n",
    "            }\n",
    "            sec_response = requests.get(url=base_url,params=payload)\n",
    "            soup = BeautifulSoup(sec_response.text,'lxml')\n",
    "            url_list = soup.findAll('filinghref')\n",
    "            html_list = []\n",
    "            # Get html version of links\n",
    "            for link in url_list:\n",
    "                link = link.string\n",
    "                if link.split(\".\")[len(link.split(\".\"))-1] == 'htm':\n",
    "                    txtlink = link + \"l\"\n",
    "                    html_list.append(txtlink)\n",
    "\n",
    "            doc_list = []\n",
    "            doc_name_list = []\n",
    "            # Get links for txt versions of files\n",
    "            for k in range(len(html_list)):\n",
    "                txt_doc = html_list[k].replace(\"-index.html\",\".txt\")\n",
    "                doc_name = txt_doc.split(\"/\")[-1]\n",
    "                doc_list.append(txt_doc)\n",
    "                doc_name_list.append(doc_name)\n",
    "                # Create dataframe of CIK, doc name, and txt link\n",
    "            df = pd.DataFrame(\n",
    "                {\n",
    "                \"cik\" : [cik]*len(html_list),\n",
    "                \"ticker\" : [ticker]*len(html_list),\n",
    "                \"txt_link\" : doc_list,\n",
    "                \"doc_name\": doc_name_list\n",
    "                }\n",
    "            )\n",
    "        except requests.exceptions.ConnectionError:\n",
    "                sleep(.1)\n",
    "        return df\n",
    "\n",
    "    # Extracts text and submission datetime from document link\n",
    "    def extract_text(link):\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            print(\"Done Link\")\n",
    "            #Parse 8-K document\n",
    "            filing = BeautifulSoup(r.content,\"html5lib\",from_encoding=\"ascii\")\n",
    "            #Extract datetime\n",
    "            try:\n",
    "                submission_dt = filing.find(\"acceptance-datetime\").string[:14]\n",
    "            except AttributeError:\n",
    "                    # Flag docs with missing data as May 1 2018 10AM\n",
    "                submission_dt = \"20180501100000\"\n",
    "            print(\"Done Date\")\n",
    "            submission_dt = datetime.datetime.strptime(submission_dt,\"%Y%m%d%H%M%S\")\n",
    "            #Extract HTML sections\n",
    "            for section in filing.findAll(\"html\"):\n",
    "                try:\n",
    "                    #Remove tables\n",
    "                    for table in section(\"table\"):\n",
    "                        table.decompose()\n",
    "                    #Convert to unicode\n",
    "                    section = unicodedata.normalize(\"NFKD\",section.text)\n",
    "                    section = section.replace(\"\\t\",\" \").replace(\"\\n\",\" \").replace(\"/s\",\" \").replace(\"\\'\",\"'\")            \n",
    "                except AttributeError:\n",
    "                    section = str(section.encode('utf-8'))\n",
    "            filing = \"\".join((section))\n",
    "        except requests.exceptions.ConnectionError:\n",
    "                sleep(10)\n",
    "        sleep(.1)\n",
    "        print (\"Done filing\")\n",
    "\n",
    "        return filing, submission_dt\n",
    "\n",
    "    def extract_item_no(document):\n",
    "        pattern = re.compile(\"Item+ +\\d+[\\:,\\.]+\\d+\\d\")\n",
    "        item_list = re.findall(pattern,document)\n",
    "        return item_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import unicodedata\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import re\n",
    "import requests\n",
    "\n",
    "class SEC_Extractor:\n",
    "    def get_doc_links(cik,ticker):\n",
    "        try:\n",
    "            base_url = \"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "            inputted_cik = cik\n",
    "            payload = {\n",
    "                \"action\" : \"getcompany\",\n",
    "                \"CIK\" : inputted_cik,\n",
    "                \"type\" : \"8-K\",\n",
    "                \"output\":\"xml\",\n",
    "                \"dateb\" : \"20180401\",\n",
    "            }\n",
    "            sec_response = requests.get(url=base_url,params=payload)\n",
    "            soup = BeautifulSoup(sec_response.text,'lxml')\n",
    "            url_list = soup.findAll('filinghref')\n",
    "            html_list = []\n",
    "            # Get html version of links\n",
    "            for link in url_list:\n",
    "                link = link.string\n",
    "                if link.split(\".\")[len(link.split(\".\"))-1] == 'htm':\n",
    "                    txtlink = link + \"l\"\n",
    "                    html_list.append(txtlink)\n",
    "\n",
    "            doc_list = []\n",
    "            doc_name_list = []\n",
    "            # Get links for txt versions of files\n",
    "            for k in range(len(html_list)):\n",
    "                txt_doc = html_list[k].replace(\"-index.html\",\".txt\")\n",
    "                doc_name = txt_doc.split(\"/\")[-1]\n",
    "                doc_list.append(txt_doc)\n",
    "                doc_name_list.append(doc_name)\n",
    "                # Create dataframe of CIK, doc name, and txt link\n",
    "            df = pd.DataFrame(\n",
    "                {\n",
    "                \"cik\" : [cik]*len(html_list),\n",
    "                \"ticker\" : [ticker]*len(html_list),\n",
    "                \"txt_link\" : doc_list,\n",
    "                \"doc_name\": doc_name_list\n",
    "                }\n",
    "            )\n",
    "        except requests.exceptions.ConnectionError:\n",
    "                sleep(.1)\n",
    "        return df\n",
    "\n",
    "    # Extracts text and submission datetime from document link\n",
    "    def extract_text(link):\n",
    "      check = 0\n",
    "      with Timeout(20):\n",
    "        try:\n",
    "                r = requests.get(link)\n",
    "                print(\"Done Link\")\n",
    "                #Parse 8-K document\n",
    "                filing = BeautifulSoup(r.content,\"html5lib\",from_encoding=\"ascii\")\n",
    "                #Extract datetime\n",
    "                try:\n",
    "                    submission_dt = filing.find(\"acceptance-datetime\").string[:14]\n",
    "                except AttributeError:\n",
    "                        # Flag docs with missing data as May 1 2018 10AM\n",
    "                    submission_dt = \"20180501100000\"\n",
    "                print(\"Done Date\")\n",
    "                submission_dt = datetime.datetime.strptime(submission_dt,\"%Y%m%d%H%M%S\")\n",
    "                #Extract HTML sections\n",
    "                for section in filing.findAll(\"html\"):\n",
    "                    try:\n",
    "                        #Remove tables\n",
    "                        for table in section(\"table\"):\n",
    "                            table.decompose()\n",
    "                        #Convert to unicode\n",
    "                        section = unicodedata.normalize(\"NFKD\",section.text)\n",
    "                        section = section.replace(\"\\t\",\" \").replace(\"\\n\",\" \").replace(\"/s\",\" \").replace(\"\\'\",\"'\")            \n",
    "                    except AttributeError:\n",
    "                        section = str(section.encode('utf-8'))\n",
    "                filing = \"\".join((section))\n",
    "        except requests.exceptions.ConnectionError:\n",
    "                sleep(10)\n",
    "        sleep(.1)\n",
    "        print (\"Done filing\")\n",
    "        return filing, submission_dt\n",
    "        check = 1\n",
    "      print(check)\n",
    "      if check == 0:\n",
    "        return (\"Passed\", \"Passed\")\n",
    "      \n",
    "    def extract_item_no(document):\n",
    "      if document != \"Passed\":\n",
    "        pattern = re.compile(\"Item+ +\\d+[\\:,\\.]+\\d+\\d\")\n",
    "        item_list = re.findall(pattern,document)\n",
    "        return item_list\n",
    "      else:\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "\n",
    "One might notice that we are using normal opening price and adjusted closing price for the index, when we are using adjusted opening and closing price for the stock prices. For the index, there is no difference between normal and adjusted price for both the GSPC and VIX. This may be likely to the fact that they may not have stock splits, dividends, and so forth to account for. So, this is not a problem.\n",
    "\n",
    "So, we felt comfortable in using adjusted opening and adjusted closing price for the stock prices. It does not create conflict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns Dataframe of document links for a given CIK\n",
    "idx = pd.Index\n",
    "class FinDataExtractor:\n",
    "    def __init__(self):\n",
    "        # S&P 500 index data downloaded from Yahoo Finance GSPC\n",
    "        self.gspc_df = pd.read_csv(\"Data/Indexes/gspc.csv\",parse_dates=['Date'],index_col=\"Date\")\n",
    "        # Get VIX index data downloaded from Yahoo Finance\n",
    "        self.vix_df = pd.read_csv(\"Data/Indexes/vix.csv\",parse_dates=['Date'],index_col=\"Date\")\n",
    "        nyse = mcal.get_calendar('NYSE')\n",
    "        self.nyse_holidays = nyse.holidays().holidays\n",
    "        self.all_tickers_data = pd.read_pickle(\"Pickles/all_tickers_data.pkl\")\n",
    "        \n",
    "#Takes datetime object and ticker string, returns price (opening or closing)\n",
    "    def get_historical_movements(self,row,period):\n",
    "        ticker,release_date = row[0],row[1]\n",
    "        print(ticker)\n",
    "        print (release_date)\n",
    "        print (type(release_date))\n",
    "       #1 Week\n",
    "        if period == \"week\":\n",
    "            e_start = release_date + datetime.timedelta(weeks=-1)\n",
    "            b_start = e_start\n",
    "\n",
    "            e_end = release_date + dateutil.relativedelta.relativedelta(days=-1)\n",
    "            b_end = e_end\n",
    "\n",
    "         #1 Month    \n",
    "        elif period == \"month\":\n",
    "            e_start = release_date + dateutil.relativedelta.relativedelta(months=-1)\n",
    "            b_start = e_start + dateutil.relativedelta.relativedelta(days=-5)\n",
    "\n",
    "            e_end = release_date + dateutil.relativedelta.relativedelta(days=-1)\n",
    "            b_end = release_date + dateutil.relativedelta.relativedelta(days=-6)\n",
    "\n",
    "        #1 Quarter\n",
    "        elif period == \"quarter\":\n",
    "            e_start = release_date + dateutil.relativedelta.relativedelta(months=-3)\n",
    "            b_start = e_start + dateutil.relativedelta.relativedelta(days=-10)\n",
    "\n",
    "            e_end = release_date + dateutil.relativedelta.relativedelta(days=-1)\n",
    "            b_end = release_date + dateutil.relativedelta.relativedelta(days=-11)\n",
    "\n",
    "        #1 Year\n",
    "        elif period == \"year\":\n",
    "            e_start = release_date + dateutil.relativedelta.relativedelta(years=-1)\n",
    "            b_start = e_start + dateutil.relativedelta.relativedelta(days=-20)\n",
    "\n",
    "            e_end = release_date + dateutil.relativedelta.relativedelta(days=-1)\n",
    "            b_end = release_date + dateutil.relativedelta.relativedelta(days=-21)\n",
    "        else:\n",
    "            print(\"error\")\n",
    "            raise KeyError\n",
    "        print(\"test\")\n",
    "        e_start = self.weekday_check(e_start)\n",
    "        b_start = self.weekday_check(b_start)\n",
    "        e_end = self.weekday_check(e_end)\n",
    "        b_end = self.weekday_check(b_end)\n",
    "\n",
    "        start_price = self.get_av_data(ticker=ticker,start_date = b_start, end_date = e_start)\n",
    "        end_price = self.get_av_data(ticker=ticker,start_date = b_end, end_date = e_end)\n",
    "        stock_change = self.calculate_pct_change(end_price,start_price)\n",
    "    \n",
    "        start_index = self.get_index_price(start_date = b_start, end_date = e_start)\n",
    "        end_index = self.get_index_price(start_date = e_start, end_date = e_end)\n",
    "        index_change =  self.calculate_pct_change(end_index,start_index)\n",
    "\n",
    "        print(start_price)\n",
    "        print(end_price)\n",
    "        print(stock_change)\n",
    "        print(start_index)\n",
    "        print(end_index)\n",
    "        print(index_change)\n",
    "        \n",
    "        normalized = stock_change - index_change\n",
    "        print (normalized)\n",
    "        return normalized\n",
    "\n",
    "    def DISABLEDget_av_data(self,ticker,start_date,end_date,market_open=False):\n",
    "        start_date = start_date.date()\n",
    "        end_date = end_date.date()\n",
    "        print (start_Date)\n",
    "        print (end_date)\n",
    "\n",
    "        try:\n",
    "            if market_open == False:\n",
    "              '''\n",
    "            Original developer had his dataset set up to slice from end date to start date\n",
    "            which caused the stock price to be set to np.nan. It may have worked fine with Alpha Vantage\n",
    "            somehow, but it does not for Wiki Data.\n",
    "            '''\n",
    "                price = self.all_tickers_data.xs(ticker,0).loc[end_date:start_date,\"adjusted_close\"].mean()\n",
    "            else:\n",
    "                price = self.all_tickers_data.xs(ticker,0).loc[end_date:start_date,\"open\"].mean()\n",
    "        except (KeyError,IndexError):\n",
    "            price = np.nan\n",
    "        print (\"price\", price)\n",
    "        return price\n",
    "      \n",
    "    def get_av_data(self, ticker,start_date,end_date,market_open=False):\n",
    "        start_date = start_date.date()\n",
    "        end_date = end_date.date()\n",
    "\n",
    "        try:\n",
    "            if market_open == False:\n",
    "              price = wikidf2.xs(ticker,0).set_index('date').loc[start_date:end_date, \"adj_close\"].mean()\n",
    "                # price = self.all_tickers_data.xs(ticker,0).loc[end_date:start_date,\"adjusted_close\"].mean()\n",
    "            else:\n",
    "              price = wikidf2.xs(ticker,0).set_index('date').loc[start_date:end_date, \"adj_open\"].mean()\n",
    "                # price = self.all_tickers_data.xs(ticker,0).loc[end_date:start_date,\"adj_open\"].mean()\n",
    "        except (KeyError,IndexError):\n",
    "            price = np.nan\n",
    "        return price\n",
    "            \n",
    "      \n",
    "\n",
    "    # Takes ticker, 8K release date, checks time of release and then calculate before and after price change\n",
    "    def get_change(self,row):\n",
    "        release_date = row['release_date']\n",
    "        print (release_date)\n",
    "        ticker = row['ticker']\n",
    "        print (ticker)\n",
    "        market_close = release_date.replace(hour=16,minute=0,second=0)\n",
    "        market_open = release_date.replace(hour=9,minute=30,second=0)\n",
    "\n",
    "    # If report is released after market hours, take change of start date close and release date open\n",
    "        if release_date > market_close:\n",
    "            start_date = release_date\n",
    "            end_date = release_date + datetime.timedelta(days=1)\n",
    "            end_date = self.weekday_check(end_date)\n",
    "            end_date = end_date\n",
    "\n",
    "            price_before_release = self.get_av_data(ticker,start_date,start_date,market_open=False)\n",
    "            price_after_release = self.get_av_data(ticker,end_date,end_date,market_open=True)\n",
    "\n",
    "            index_before_release = self.get_index_price(start_date,start_date,market_open=False)\n",
    "            index_after_release = self.get_index_price(end_date,end_date,market_open=True)\n",
    "\n",
    "            try:\n",
    "                vix = self.vix_df.loc[self.vix_df.index == np.datetime64(start_date.date()),\"Adj Close\"][0].item()\n",
    "            except IndexError:\n",
    "                vix = np.nan\n",
    "\n",
    "        # If report is released before market hours, take change of start date's close and release date's open\n",
    "        elif release_date < market_open:\n",
    "            start_date = release_date + datetime.timedelta(days=-1)\n",
    "            start_date = self.weekday_check(start_date)\n",
    "            end_date = release_date\n",
    "\n",
    "            price_before_release = self.get_av_data(ticker,start_date,start_date,market_open=False)\n",
    "            price_after_release = self.get_av_data(ticker,end_date,end_date,market_open=True) \n",
    "\n",
    "            index_before_release = self.get_index_price(start_date,start_date,market_open=False)\n",
    "            index_after_release = self.get_index_price(end_date,end_date,market_open=True)\n",
    "            try:\n",
    "                vix = self.vix_df.loc[self.vix_df.index == np.datetime64(start_date.date()),\"Adj Close\"][0].item()\n",
    "            except IndexError:\n",
    "                vix = np.nan\n",
    "        # If report is released during market hours, use market close\n",
    "        else:\n",
    "            start_date = release_date\n",
    "            end_date = release_date\n",
    "            price_before_release = self.get_av_data(ticker,start_date,start_date,market_open=True)\n",
    "            price_after_release = self.get_av_data(ticker,end_date,end_date,market_open=False)\n",
    "\n",
    "            index_before_release = self.get_index_price(start_date,start_date,market_open=True)\n",
    "            index_after_release = self.get_index_price(end_date,end_date,market_open=False)\n",
    "            \n",
    "            try:\n",
    "                vix = self.vix_df.loc[self.vix_df.index == np.datetime64(start_date.date()),\"Open\"][0].item()\n",
    "            except IndexError:\n",
    "                vix = np.nan\n",
    "        print(price_after_release)\n",
    "        print(price_before_release)\n",
    "        price_pct_change = self.calculate_pct_change(price_after_release,price_before_release)\n",
    "        index_pct_change = self.calculate_pct_change(index_after_release,index_before_release)\n",
    "        normalized_change = price_pct_change - index_pct_change\n",
    "        print (normalized_change)\n",
    "        print (vix)\n",
    "        return normalized_change, vix\n",
    "\n",
    "    def get_index_price(self,start_date,end_date,market_open=False):\n",
    "        try:\n",
    "            if market_open == True:\n",
    "                price = self.gspc_df.loc[(self.gspc_df.index >= np.datetime64(start_date.date())) & \n",
    "                                 (self.gspc_df.index <= np.datetime64(end_date)),\"Open\"].mean()\n",
    "            else:\n",
    "                price = self.gspc_df.loc[(self.gspc_df.index >= np.datetime64(start_date.date())) & \n",
    "                                 (self.gspc_df.index <= np.datetime64(end_date)),\"Adj Close\"].mean()\n",
    "        except IndexError:\n",
    "                price = np.nan\n",
    "        return price\n",
    "\n",
    "    def calculate_pct_change(self,end_value,start_value):\n",
    "        pct_change = (end_value - start_value) / start_value\n",
    "        pct_change = round(pct_change,4) * 100\n",
    "        return pct_change\n",
    "\n",
    "    def weekday_check(self,date):  \n",
    "        while date.isoweekday() > 5 or date.date() in self.nyse_holidays:\n",
    "            date = date + datetime.timedelta(days=-1)\n",
    "        return date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get S&P 500 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table of the S&P 500 tickers, CIK, and industry from Wikipedia\n",
    "wiki_url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "cik_df = pd.read_html(wiki_url,header=0,index_col=0)[0]\n",
    "cik_df['GICS Sector'] = cik_df['GICS Sector'].astype(\"category\")\n",
    "cik_df['GICS Sub Industry'] = cik_df['GICS Sector'].astype(\"category\")\n",
    "cik_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get List of 8K links from SEC Edgar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sec_ext = SEC_Extractor\n",
    "no_parts = 2\n",
    "part_no = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "company_list = cik_df['CIK'].to_dict()\n",
    "for (ticker,cik) in tqdm(company_list.items()):\n",
    "    df_list.append(sec_ext.get_doc_links(cik,ticker))\n",
    "doc_links_df = pd.concat(df_list,axis=0)\n",
    "doc_links_df = doc_links_df.set_index(\"ticker\").join(cik_df['GICS Sector']).join(cik_df['GICS Sub Industry']).reset_index().rename(columns={\"index\":\"ticker\"})\n",
    "doc_links_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_links_df.to_csv(\"doc_links_df.csv\")\n",
    "doc_links_df.to_pickle(\"Pickles/doc_links_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Download 8Ks & Stock Movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_df = pd.read_pickle(\"Pickles/doc_links_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = len(pd.read_csv(\"Data/texts_example1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_df = crawled_df.iloc[start:, :] # Skip 842. We also miss out on 840 and 841"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while part_no > no_parts:\n",
    "    no_parts = int(input(\"Split data into how many parts?\"))\n",
    "    part_no = int(input(\"Which part is this?\"))\n",
    "    \n",
    "chunksize = int(input(\"Number of rows to process at once (10 to 50 recommended)\"))\n",
    "\n",
    "#Load pickle\n",
    "# crawled_df = np.array_split(pd.read_pickle(\"Pickles/doc_links_df.pkl\"),no_parts)[part_no-1]\n",
    "crawled_df =  np.array_split(crawled_df, no_parts)[part_no-1]\n",
    "crawled_len = len(crawled_df['txt_link'])\n",
    "\n",
    "\n",
    "df_list = []\n",
    "for i, df in tqdm(enumerate(np.array_split(crawled_df,chunks))):\n",
    "    print(i)\n",
    "    df['text'], df['release_date'] = zip(*df['txt_link'].apply(sec_ext.extract_text))\n",
    "    # df['text'], df['release_date'] = sec_ext.extract_text(df['txt_link'])\n",
    "    \n",
    "    df['items'] = df['text'].map(sec_ext.extract_item_no)\n",
    "    if not os.path.isfile(\"Data/texts_example{}.csv\".format(part_no)): #If no file exists, create one with header\n",
    "        df.to_csv(\"Data/texts_example{}.csv\".format(part_no),chunksize=chunksize,)\n",
    "    else: # else it exists so append without writing the header\n",
    "        df.to_csv(\"Data/texts_example{}.csv\".format(part_no),mode=\"a\",header=False,chunksize=chunksize)       \n",
    "    df_list.append(df)\n",
    "    del df\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        gc.collect()\n",
    "df = pd.concat(df_list)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_df = pd.read_pickle(\"Pickles/doc_links_df.pkl\")\n",
    "crawled_len = len(crawled_df['txt_link'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/texts_example1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = copy.deepcopy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_dict = cik_df['Symbol'].to_dict()\n",
    "cik_dict = {v: k for v, k in cik_dict.items()}\n",
    "df3['ticker'] = df3['ticker'].map(cik_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidf = pd.read_csv('WIKI_PRICES_212b326a081eacca455e13140d7bb9db.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidf['date']  = pd.to_datetime(wikidf['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidf[(wikidf[\"ticker\"] == \"MMM\")].iloc[-10:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidf[(wikidf[\"date\"] == df['release_date'][0].normalize())].iloc[-10:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidf2 = wikidf.set_index('ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidf2.xs('MMM',0).iloc[-200:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(wikidf2.iloc[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidf2.xs('MMM',0).set_index('date').loc[pd.to_datetime('1970-01-02'):pd.to_datetime('1970-01-06'), \"adj_open\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidf2.xs('MMM',0).set_index('date').loc[pd.to_datetime('2018-02-20'), \"adj_open\"].mean() # Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  wikidf[(wikidf[\"ticker\"] == \"MMM\" and wikidf[\"date\"] == df['release_date'][0].normalize())] \n",
    "except ValueError:\n",
    "  wikidf[(wikidf[\"ticker\"] == \"MMM\" and wikidf[\"date\"] == wikidf['release_date'][-1].normalize())] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = copy.deepcopy(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask\n",
    "df2 = df[(df['text'] != \"Passed\")]\n",
    "df2 = df2[(df2['release_date'] != \"Passed\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['release_date']  = pd.to_datetime(df2['release_date'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows flagged where no date was found\n",
    "df2 = df2.loc[~(df2['release_date'] >= pd.datetime(year=2018,month=5,day=1))]\n",
    "df2 = df2.drop_duplicates(subset=\"doc_name\")\n",
    "df2.index.names = ['ticker']\n",
    "# df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = copy.deepcopy(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['release_date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from FinDataExtractor import FinDataExtractor\n",
    "fin_data = FinDataExtractor()\n",
    "## Load pickle of ticker, date, and doc number\n",
    "\n",
    "\n",
    "tempContainer = df[['ticker','release_date']].apply(fin_data.get_change,axis=1)\n",
    "tempContainer.rename({\"ticker\": \"price_change\", \"release_date\": \"VIX\"}, axis = 1, inplace = True)\n",
    "df = df.join(tempContainer)\n",
    "print (\"pc done\", df.head())\n",
    "df['rm_week'] = df[['ticker','release_date']].apply(fin_data.get_historical_movements,period=\"week\",axis=1)\n",
    "print (\"week done\", df.head())\n",
    "df['rm_month'] = df[['ticker','release_date']].apply(fin_data.get_historical_movements,period=\"month\",axis=1)\n",
    "print (\"month done\", df.head())\n",
    "df['rm_qtr'] = df[['ticker','release_date']].apply(fin_data.get_historical_movements,period=\"quarter\",axis=1)\n",
    "print (\"quart done\", df.head())\n",
    "df['rm_year'] = df[['ticker','release_date']].apply(fin_data.get_historical_movements,period=\"year\",axis=1)\n",
    "print (\"year done\", df.head())\n",
    "df[\"signal\"] = df['price_change'].map(lambda x: np.nan if math.isnan(x) else (\"stay\" if -1<x<1  else (\"up\" if x>1 else \"down\")))\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
